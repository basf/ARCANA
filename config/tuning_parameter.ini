; Any of the parameters can deleted if not applicable for the used model. Choosing a step for the parameter search is optional.
[huber_loss]
; The delta value for Huber loss
beta =

[optimizer]
; The range of learning rates to explore during tuning [start, stop, step]
learning_rate =  ; float
; The weight decay regularization term
weight_decay =  ; float

[scheduler_reduced]
; The factor by which the learning rate is reduced
factor_reduced =

[model_parameters]
; The range of number of training epochs to explore [start, stop, step]
number_of_epochs =  ; int
; The range of hidden layer dimensionalities to explore [start, stop, step]
hidden_dim =  ; int
; The batch size for training
batch_size =  ; int
; Whether the LSTM is bidirectional or not
bidirectional =  ; categorical (True or False)
; The range of dropout rates for the output layer to explore [start, stop, step]
output_dropout =  ; float
; The range of window lengths for input sequences to explore [start, stop, step]
window_length =  ; int
; The range of warmup steps for learning rate scheduling to explore [start, stop, step]
warmup_steps =  ; int

[encoder]
; The range of dropout rates for the encoder to explore [start, stop, step]
dropout_encoder =  ; float
; The range of layer counts for the encoder to explore [start, stop, step]
num_layers_encoder =  ; int

[decoder]
; The range of dropout rates for the decoder to explore [start, stop, step]
dropout_decoder =  ; float
; The number of layers in the decoder LSTM
num_layers_decoder =  ; int

[multihead_attention]
; The range of attention heads in the encoder to explore [start, stop, step]
nhead_encoder =  ; int
; The range of attention heads in the decoder to explore [start, stop, step]
nhead_decoder =  ; int

[teacher_forcing]
; The range of decay strides for teacher forcing ratio to explore [start, stop, step]
decay_stride =  ; float